{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7867278be8149589947607151fd6a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import imodelsx.process_results\n",
    "import sys\n",
    "import datasets\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import clin.eval\n",
    "import clin.modules.prune\n",
    "import clin.modules.evidence\n",
    "import clin.modules.omission\n",
    "import clin.modules.status\n",
    "import clin.llm\n",
    "import clin.parse\n",
    "from collections import defaultdict\n",
    "import openai\n",
    "openai.api_key_path = '/home/chansingh/.OPENAI_KEY'\n",
    "\n",
    "sys.path.append('../experiments/')\n",
    "results_dir = '../results/'\n",
    "\n",
    "r = imodelsx.process_results.get_results_df(results_dir, use_cached=True)\n",
    "\n",
    "# get data for eval\n",
    "dset = datasets.load_dataset('mitclinicalml/clinical-ie', 'medication_status')\n",
    "df_val = pd.DataFrame.from_dict(dset['validation'])\n",
    "df = pd.DataFrame.from_dict(dset['test'])\n",
    "# df = pd.concat([val, test])\n",
    "nums = np.arange(len(df)).tolist()\n",
    "np.random.default_rng(seed=13).shuffle(nums)\n",
    "dfe = df.iloc[nums]\n",
    "n = len(dfe)\n",
    "\n",
    "# (\n",
    "#     r.groupby(['checkpoint', 'n_shots'])[['f1', 'recall', 'precision']].mean()\n",
    "#     .style.format(precision=3).background_gradient(cmap='Blues')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>metric</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verification</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ev</th>\n",
       "      <td>0.918</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original</th>\n",
       "      <td>0.919</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ov</th>\n",
       "      <td>0.906</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ov_pv</th>\n",
       "      <td>0.936</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ov_pv_ev</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pv</th>\n",
       "      <td>0.926</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "metric           f1  precision  recall\n",
       "verification                          \n",
       "ev            0.918      0.918   0.918\n",
       "original      0.919      0.918   0.921\n",
       "ov            0.906      0.870   0.944\n",
       "ov_pv         0.936      0.946   0.926\n",
       "ov_pv_ev      0.938      0.949   0.926\n",
       "pv            0.926      0.948   0.906"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = r[[c for c in r.columns if '___' in c]]\n",
    "# create multindex columns by splitting on '___'\n",
    "rc = rc.rename(columns={c: tuple(c.split('___')) for c in rc.columns})\n",
    "\n",
    "# convert tuple column names to multiindex\n",
    "rc.columns = pd.MultiIndex.from_tuples(rc.columns)\n",
    "rc = rc.T.reset_index()\n",
    "rc = rc.rename(columns={\n",
    "    'level_0': 'metric',\n",
    "    'level_1': 'verification',\n",
    "}).pivot_table(index='verification', columns='metric', values=0).round(3)\n",
    "\n",
    "rc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test verifiers on medication extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get original\n",
    "row = r[(r.n_shots == 5) * (r.checkpoint == 'text-davinci-003')].iloc[0]\n",
    "extracted_strs_orig = row['resps']\n",
    "med_status_dict_list_orig = [clin.parse.parse_response_medication_list(extracted_strs_orig[i]) for i in range(n)]\n",
    "llm_verify = clin.llm.get_llm('text-davinci-003')\n",
    "\n",
    "ov = clin.modules.omission.OmissionVerifier()\n",
    "pv = clin.modules.prune.PruneVerifier()\n",
    "ev = clin.modules.evidence.EvidenceVerifier(n_shots_neg=1, n_shots_pos=1)\n",
    "sv = clin.modules.status.StatusVerifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply omission verifier\n",
    "med_status_dict_list_ov = [ov(dfe.iloc[i]['snippet'], bulleted_str=extracted_strs_orig[i], llm=llm_verify, verbose=False) for i in tqdm(range(n))]\n",
    "\n",
    "# apply prune verifier\n",
    "med_status_dict_list_pv = [pv(dfe.iloc[i]['snippet'], bulleted_str=extracted_strs_orig[i], llm=llm_verify, verbose=False) for i in tqdm(range(n))]\n",
    "\n",
    "# apply evidence verifier\n",
    "med_status_and_evidence = [ev(snippet=dfe.iloc[i]['snippet'], bulleted_str=extracted_strs_orig[i], llm=llm_verify) for i in tqdm(range(n))]\n",
    "med_status_dict_list_ev = [med_status_and_evidence[i][0] for i in range(n)]\n",
    "med_evidence_dict_list_ev = [med_status_and_evidence[i][1] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# apply sequentially\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m med_status_dict_list_ov_ \u001b[39m=\u001b[39m [ov(dfe\u001b[39m.\u001b[39miloc[i][\u001b[39m'\u001b[39m\u001b[39msnippet\u001b[39m\u001b[39m'\u001b[39m], bulleted_str\u001b[39m=\u001b[39mextracted_strs_orig[i], llm\u001b[39m=\u001b[39mllm, lower\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n))]\n\u001b[1;32m      3\u001b[0m bulleted_str_list_ov_ \u001b[39m=\u001b[39m [clin\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39mmedication_dict_to_bullet_str(med_status_dict_list_ov_[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n)]\n\u001b[1;32m      5\u001b[0m med_status_dict_list_pv_ \u001b[39m=\u001b[39m [pv(dfe\u001b[39m.\u001b[39miloc[i][\u001b[39m'\u001b[39m\u001b[39msnippet\u001b[39m\u001b[39m'\u001b[39m], bulleted_str\u001b[39m=\u001b[39mbulleted_str_list_ov_[i], llm\u001b[39m=\u001b[39mllm, lower\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n))]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "# apply sequential verifiers\n",
    "med_status_dict_list_ov_ = [ov(dfe.iloc[i]['snippet'], bulleted_str=extracted_strs_orig[i], llm=llm_verify, lower=False) for i in tqdm(range(n))]\n",
    "bulleted_str_list_ov_ = [clin.parse.medication_dict_to_bullet_str(med_status_dict_list_ov_[i]) for i in range(n)]\n",
    "\n",
    "med_status_dict_list_pv_ = [pv(dfe.iloc[i]['snippet'], bulleted_str=bulleted_str_list_ov_[i], llm=llm_verify, lower=False) for i in tqdm(range(n))]\n",
    "bulleted_str_list_pv_ = [clin.parse.medication_dict_to_bullet_str(med_status_dict_list_pv_[i]) for i in range(n)]\n",
    "\n",
    "med_status_and_evidence_ = [ev(snippet=dfe.iloc[i]['snippet'], bulleted_str=bulleted_str_list_pv_[i], llm=llm_verify) for i in tqdm(range(n))]\n",
    "med_status_dict_list_ev_ = [med_status_and_evidence_[i][0] for i in range(n)]\n",
    "med_evidence_dict_list_ev_ = [med_status_and_evidence_[i][1] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_status_results = {\n",
    "    'original': med_status_dict_list_orig,\n",
    "    'omission': med_status_dict_list_ov,\n",
    "    'prune': med_status_dict_list_pv,\n",
    "    'evidence': med_status_dict_list_ev,\n",
    "    'omission + prune': med_status_dict_list_pv_,\n",
    "    'omission + prune + evidence': med_status_dict_list_ev_,\n",
    "} \n",
    "mets_dict = defaultdict(list)\n",
    "for k in med_status_results.keys():\n",
    "    mets_dict_single = clin.eval.calculate_metrics(med_status_results[k], dfe, verbose=False)\n",
    "    for k_met in mets_dict_single.keys():\n",
    "        mets_dict[k_met].append(mets_dict_single[k_met])\n",
    "df = pd.DataFrame.from_dict(mets_dict).round(3)[['f1', 'recall', 'precision']]\n",
    "df.index = med_status_results.keys()\n",
    "df.style.format(precision=3).background_gradient(cmap='Blues')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mets = clin.eval.calculate_metrics(med_status_dict_list_pvs[0], dfe, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on medication status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = med_status_dict_list_pvs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulleted_str_list_d = [clin.parse.medication_dict_to_bullet_str(d[i]) for i in tqdm(range(n))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = clin.verifiers.evidence.EvidenceVerifier(n_shots_neg=2, n_shots_pos=0)\n",
    "# ev = clin.verifiers.evidence.EvidenceVerifier(n_shots_neg=0, n_shots_pos=2)\n",
    "med_status_dict_list_ev = []\n",
    "med_evidence_dict_list_ev = []\n",
    "for i in tqdm(range(n)):\n",
    "    med_status_dict, med_evidence_dict = ev(\n",
    "        snippet=dfe.iloc[i]['snippet'], bulleted_str=bulleted_str_list_d[i], llm=llm)\n",
    "    med_status_dict_list_ev.append(med_status_dict)\n",
    "    med_evidence_dict_list_ev.append(med_evidence_dict)\n",
    "med_status_dict_list_ev_pruned = [\n",
    "    {\n",
    "        k: v for k, v in med_status_dict_list_ev[i].items()\n",
    "        if not med_evidence_dict_list_ev[i][k] == 'no evidence'\n",
    "        # and med_evidence_dict_list_ev[i][k] in dfe.iloc[i]['snippet'].lower()\n",
    "    }\n",
    "    for i in range(n)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(n)):\n",
    "    print(i)\n",
    "    sv(dfe.iloc[i]['snippet'],\n",
    "                              med_status_dict=med_status_dict_list_ev[i],\n",
    "                                med_evidence_dict=med_evidence_dict_list_ev[i],\n",
    "                                llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_status_dict_list_sv = [sv(dfe.iloc[i]['snippet'],\n",
    "                              med_status_dict=med_status_dict_list_ev[i],\n",
    "                                med_evidence_dict=med_evidence_dict_list_ev[i],\n",
    "                                llm=llm) for i in tqdm(range(n))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add medication status eval\n",
    "med_status_dicts_list = [[clin.parse.parse_response_medication_list(r.iloc[i]['resps'][j]) for j in range(n)] for i in range(len(r))]\n",
    "accs_cond, f1s_macro_cond = clin.eval.eval_medication_status(med_status_dicts_list, dfe)\n",
    "r['acc_cond'] = accs_cond\n",
    "r['f1_macro_cond'] = f1s_macro_cond\n",
    "\n",
    "(\n",
    "    r.groupby(['checkpoint', 'n_shots'])[['f1', 'recall', 'precision', 'acc_cond', 'f1_macro_cond']].mean()\n",
    "    .style.format(precision=3).background_gradient(cmap='Blues')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_status_dicts_list = [[clin.parse.parse_response_medication_list(r.iloc[i]['resps'][j]) for j in range(n)] for i in range(len(r))]\n",
    "med_status_dicts_list += list(PREDS_DICT.values()) + [d_pruned]\n",
    "accs_cond, f1s_macro_cond = clin.eval.eval_medication_status(med_status_dicts_list, dfe)\n",
    "idx = list((r.checkpoint + ' ' + r.n_shots.astype(str)).values) + list(PREDS_DICT.keys()) + ['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.DataFrame.from_dict({'accs_cond': accs_cond, 'f1s_macro_cond': f1s_macro_cond}, orient='index', columns=idx).T\n",
    "    .style.format(precision=3).background_gradient(cmap='Blues')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".embgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
