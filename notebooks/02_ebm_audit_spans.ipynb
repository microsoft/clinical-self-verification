{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1646.07it/s]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import clin.llm\n",
    "import clin.parse\n",
    "from typing import List\n",
    "results_dir = '../results/'\n",
    "from clin.config import PATH_REPO\n",
    "import clin.eval.ebm\n",
    "import clin.eval.eval\n",
    "from clin.modules import ebm\n",
    "import joblib\n",
    "import imodelsx.process_results\n",
    "from IPython.display import HTML\n",
    "import clin.viz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1773.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# get human spans\n",
    "df_spans = joblib.load(join(PATH_REPO, 'data', 'ebm', 'ebm_interventions_spans.pkl'))\n",
    "nums = np.arange(100).tolist()\n",
    "np.random.default_rng(seed=13).shuffle(nums)\n",
    "dfe_spans = df_spans.iloc[nums]\n",
    "\n",
    "# get predicted evidence\n",
    "r = imodelsx.process_results.get_results_df(results_dir, use_cached=False)\n",
    "r = r[r.dataset_name == 'ebm']\n",
    "row = r[(r.n_shots == 5) * (r.checkpoint == 'text-davinci-003')].iloc[0]\n",
    "d_evidence = row['dict_evidence_ov_pv_ev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1866145/1503278224.py:66: RuntimeWarning: Mean of empty slice\n",
      "  mean_matches.append(np.nanmean(matches))\n",
      "/tmp/ipykernel_1866145/1503278224.py:67: RuntimeWarning: Mean of empty slice\n",
      "  mean_num_tokens.append(np.nanmean(num_toks))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8212752430114761, 2.038433998384656, 0.07789028384988256)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_mean_matches(d_evidence, dfe_spans):\n",
    "    \"\"\"Finds mean number of times evidence span from llm contains a token from the human span.\n",
    "    Mean is taken for each document and then averaged over all documents.\n",
    "    Baseline is probability that any token falls into a human span.\n",
    "    \"\"\"\n",
    "\n",
    "    # single example\n",
    "    mean_matches = []\n",
    "    mean_matches_random = []\n",
    "    mean_num_tokens = []\n",
    "    for i in range(len(dfe_spans)):\n",
    "        span = dfe_spans.iloc[i]\n",
    "        doc = span['doc'].lower()\n",
    "        toks = [tok.lower() for tok in span['toks_list']]\n",
    "        annot = span['annot_list']\n",
    "        # color_str = clin.viz.colorize(span['toks_list'], span['annot_list'], char_width_max=60, title=str(i) + \" \" + span['doc_id'])\n",
    "        # display(HTML(color_str))\n",
    "\n",
    "        # given reference text and set of tokens, find starting index of each token\n",
    "        def _find_token_idxs(doc, toks):\n",
    "            starts = []\n",
    "            ends = []\n",
    "            for tok in toks:\n",
    "                idx = doc.find(tok, ends[-1] if len(ends) > 0 else 0)\n",
    "                if idx == -1:\n",
    "                    print('ERROR: token not found:', tok)\n",
    "                    return None\n",
    "                starts.append(idx)\n",
    "                ends.append(idx + len(tok))\n",
    "            \n",
    "            # check that idxs are strictly increasing\n",
    "            for i in range(1, len(starts)):\n",
    "                if starts[i] <= starts[i - 1]:\n",
    "                    print('ERROR: idxs not strictly increasing')\n",
    "                    return None\n",
    "            return starts, ends\n",
    "        starts, ends = _find_token_idxs(doc, toks)\n",
    "\n",
    "        def _get_overlapping_token_idxs(start: int, end: int, starts: List[int], ends: List[int]):\n",
    "            \"\"\"\n",
    "            Given a span [start, end), find the indices of all tokens that overlap with the span.\n",
    "            \"\"\"\n",
    "            idxs = []\n",
    "            for i in range(len(starts)):\n",
    "                if start < ends[i] and end > starts[i]:\n",
    "                    # print(start, end, starts[i], ends[i])\n",
    "                    idxs.append(i)\n",
    "            return idxs\n",
    "\n",
    "        evidence = d_evidence[i]\n",
    "        matches = []\n",
    "        num_toks = []\n",
    "        for intervention in evidence:\n",
    "            # idx = doc.find(intervention.lower())\n",
    "            \n",
    "            # search over all matches\n",
    "            idxs_match = [m.start() for m in re.finditer(intervention.lower(), doc)]\n",
    "            for idx_match in idxs_match:\n",
    "                # print(intervention, doc[idx: idx + len(intervention)])\n",
    "                tok_idxs = _get_overlapping_token_idxs(idx_match, idx_match + len(intervention), starts, ends)\n",
    "                # print(np.array(toks)[tok_idxs])\n",
    "                # print(intervention, tok_idxs, annot[tok_idxs])\n",
    "                matches.append(np.any(annot[tok_idxs] > 0))\n",
    "                num_toks.append(len(tok_idxs))\n",
    "        mean_matches_random.append(np.nanmean(annot > 0))\n",
    "        mean_matches.append(np.nanmean(matches))\n",
    "        mean_num_tokens.append(np.nanmean(num_toks))\n",
    "    return np.nanmean(mean_matches), np.nanmean(mean_num_tokens), np.nanmean(mean_matches_random)\n",
    "\n",
    "calculate_mean_matches(d_evidence, dfe_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1866145/1503278224.py:66: RuntimeWarning: Mean of empty slice\n",
      "  mean_matches.append(np.nanmean(matches))\n",
      "/tmp/ipykernel_1866145/1503278224.py:67: RuntimeWarning: Mean of empty slice\n",
      "  mean_num_tokens.append(np.nanmean(num_toks))\n",
      "/tmp/ipykernel_1866145/1503278224.py:66: RuntimeWarning: Mean of empty slice\n",
      "  mean_matches.append(np.nanmean(matches))\n",
      "/tmp/ipykernel_1866145/1503278224.py:67: RuntimeWarning: Mean of empty slice\n",
      "  mean_num_tokens.append(np.nanmean(num_toks))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random baseline 0.07789028384988256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1866145/1503278224.py:66: RuntimeWarning: Mean of empty slice\n",
      "  mean_matches.append(np.nanmean(matches))\n",
      "/tmp/ipykernel_1866145/1503278224.py:67: RuntimeWarning: Mean of empty slice\n",
      "  mean_num_tokens.append(np.nanmean(num_toks))\n"
     ]
    }
   ],
   "source": [
    "r['Match accuracy'] = r.apply(lambda row: calculate_mean_matches(row['dict_evidence_ov_pv_ev'], dfe_spans)[0], axis=1)\n",
    "r['Span length'] = r.apply(lambda row: calculate_mean_matches(row['dict_evidence_ov_pv_ev'], dfe_spans)[1], axis=1)\n",
    "print('random baseline', r.apply(lambda row: calculate_mean_matches(row['dict_evidence_ov_pv_ev'], dfe_spans)[2], axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Match accuracy</th>\n",
       "      <th>Span length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checkpoint</th>\n",
       "      <th>n_shots</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt-4-0314</th>\n",
       "      <th>1</th>\n",
       "      <td>0.904</td>\n",
       "      <td>2.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.898</td>\n",
       "      <td>2.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">text-davinci-003</th>\n",
       "      <th>1</th>\n",
       "      <td>0.822</td>\n",
       "      <td>2.349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.827</td>\n",
       "      <td>2.020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Match accuracy  Span length\n",
       "checkpoint       n_shots                             \n",
       "gpt-4-0314       1                 0.904        2.386\n",
       "                 5                 0.898        2.263\n",
       "text-davinci-003 1                 0.822        2.349\n",
       "                 5                 0.827        2.020"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.groupby(['checkpoint', 'n_shots'])[['Match accuracy', 'Span length']].mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".embgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
