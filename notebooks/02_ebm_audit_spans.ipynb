{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import clin.llm\n",
    "import clin.parse\n",
    "from typing import List, Dict\n",
    "results_dir = '../results/'\n",
    "from clin.config import PATH_REPO\n",
    "import clin.eval.ebm\n",
    "import clin.eval.eval\n",
    "from clin.modules import ebm\n",
    "import joblib\n",
    "import imodelsx.process_results\n",
    "from IPython.display import HTML\n",
    "import clin.viz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 1769.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# get human spans\n",
    "df_spans = joblib.load(join(PATH_REPO, 'data', 'ebm', 'ebm_interventions_spans.pkl'))\n",
    "nums = np.arange(100).tolist()\n",
    "np.random.default_rng(seed=13).shuffle(nums)\n",
    "dfe_spans = df_spans.iloc[nums]\n",
    "\n",
    "# get predicted evidence\n",
    "r = imodelsx.process_results.get_results_df(results_dir, use_cached=False)\n",
    "r = r[r.dataset_name == 'ebm']\n",
    "row = r[(r.n_shots == 5) * (r.checkpoint == 'text-davinci-003')].iloc[0]\n",
    "\n",
    "# get common keys across each list\n",
    "common_keys = [\n",
    "    set.intersection(\n",
    "        *[set(r[\"dict_evidence_ov_pv_ev\"].iloc[i][j].keys()) for i in range(len(r))]\n",
    "    )\n",
    "    for j in range(100)\n",
    "]\n",
    "r['dict_evidence_common'] = r.apply(lambda x: [{k: x['dict_evidence_ov_pv_ev'][i][k] for k in common_keys[i]} for i in range(100)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_matches(d_evidence: List[Dict[str, str]], dfe_spans):\n",
    "    \"\"\"Finds mean number of times evidence span from llm contains a token from the human span.\n",
    "    Mean is taken for each document and then averaged over all documents.\n",
    "    Baseline is probability that any token falls into a human span.\n",
    "    \"\"\"\n",
    "\n",
    "    # single example\n",
    "    mean_matches = []\n",
    "    mean_num_tokens = []\n",
    "    for i in range(len(dfe_spans)):\n",
    "        span = dfe_spans.iloc[i]\n",
    "        doc = span['doc'].lower()\n",
    "        toks = [tok.lower() for tok in span['toks_list']]\n",
    "        annot = span['annot_list']\n",
    "        # color_str = clin.viz.colorize(span['toks_list'], span['annot_list'], char_width_max=60, title=str(i) + \" \" + span['doc_id'])\n",
    "        # display(HTML(color_str))\n",
    "\n",
    "        # given reference text and set of tokens, find starting index of each token\n",
    "        def _find_token_idxs(doc, toks):\n",
    "            starts = []\n",
    "            ends = []\n",
    "            for tok in toks:\n",
    "                idx = doc.find(tok, ends[-1] if len(ends) > 0 else 0)\n",
    "                if idx == -1:\n",
    "                    print('ERROR: token not found:', tok)\n",
    "                    return None\n",
    "                starts.append(idx)\n",
    "                ends.append(idx + len(tok))\n",
    "            \n",
    "            # check that idxs are strictly increasing\n",
    "            for i in range(1, len(starts)):\n",
    "                if starts[i] <= starts[i - 1]:\n",
    "                    print('ERROR: idxs not strictly increasing')\n",
    "                    return None\n",
    "            return starts, ends\n",
    "        starts, ends = _find_token_idxs(doc, toks)\n",
    "\n",
    "        def _get_overlapping_token_idxs(start: int, end: int, starts: List[int], ends: List[int]):\n",
    "            \"\"\"\n",
    "            Given a span [start, end), find the indices of all tokens that overlap with the span.\n",
    "            \"\"\"\n",
    "            idxs = []\n",
    "            for i in range(len(starts)):\n",
    "                if start < ends[i] and end > starts[i]:\n",
    "                    # print(start, end, starts[i], ends[i])\n",
    "                    idxs.append(i)\n",
    "            return idxs\n",
    "\n",
    "        matches = []\n",
    "        num_toks = []\n",
    "        for intervention_name, intervention_evidence in d_evidence[i].items():\n",
    "            s = re.escape(intervention_evidence.lower())\n",
    "            # s = intervention_name.lower()\n",
    "\n",
    "            # search over all matches\n",
    "            idxs_match = [m.start() for m in re.finditer(s, doc)]\n",
    "            for idx_match in idxs_match:\n",
    "                tok_idxs = _get_overlapping_token_idxs(idx_match, idx_match + len(s), starts, ends)\n",
    "                matches.append(np.any(annot[tok_idxs] > 0))\n",
    "                # num_toks.append(len(tok_idxs))\n",
    "                num_toks.append(len(s.split()))\n",
    "            # else:\n",
    "                # matches.append(0)\n",
    "                # num_toks.append(len(s.split())\n",
    "        mean_matches.append(np.nanmean(matches))\n",
    "        mean_num_tokens.append(np.nanmean(num_toks))\n",
    "    return np.nanmean(mean_matches), np.nanmean(mean_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1866145/1189457189.py:65: RuntimeWarning: Mean of empty slice\n",
      "  mean_matches.append(np.nanmean(matches))\n",
      "/tmp/ipykernel_1866145/1189457189.py:66: RuntimeWarning: Mean of empty slice\n",
      "  mean_num_tokens.append(np.nanmean(num_toks))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random baseline 0.03757215007215007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1866145/1189457189.py:65: RuntimeWarning: Mean of empty slice\n",
      "  mean_matches.append(np.nanmean(matches))\n",
      "/tmp/ipykernel_1866145/1189457189.py:66: RuntimeWarning: Mean of empty slice\n",
      "  mean_num_tokens.append(np.nanmean(num_toks))\n"
     ]
    }
   ],
   "source": [
    "r['Span overlap accuracy'] = r.apply(lambda row: calculate_mean_matches(row['dict_evidence_ov_pv_ev'], dfe_spans)[0], axis=1)\n",
    "r['Span length'] = r.apply(lambda row: calculate_mean_matches(row['dict_evidence_ov_pv_ev'], dfe_spans)[1], axis=1)\n",
    "print('random baseline', np.concatenate(dfe_spans['annot_list'].values).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Span overlap accuracy</th>\n",
       "      <th>Span length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checkpoint</th>\n",
       "      <th>n_shots</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt-4-0314</th>\n",
       "      <th>1</th>\n",
       "      <td>0.92</td>\n",
       "      <td>7.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.92</td>\n",
       "      <td>8.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">text-davinci-003</th>\n",
       "      <th>1</th>\n",
       "      <td>0.85</td>\n",
       "      <td>7.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.85</td>\n",
       "      <td>7.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Span overlap accuracy  Span length\n",
       "checkpoint       n_shots                                    \n",
       "gpt-4-0314       1                         0.92         7.98\n",
       "                 5                         0.92         8.34\n",
       "text-davinci-003 1                         0.85         7.80\n",
       "                 5                         0.85         7.03"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.groupby(['checkpoint', 'n_shots'])[['Span overlap accuracy', 'Span length']].mean().round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".embgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
