{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import clin.eval.med_status\n",
    "import datasets\n",
    "import clin.parse\n",
    "import openai\n",
    "openai.api_key_path = '/home/chansingh/.OPENAI_KEY'\n",
    "results_dir = '../results/'\n",
    "# results_dir = '../results_tmp/'\n",
    "from clin.config import PATH_REPO\n",
    "import imodelsx.process_results\n",
    "r = imodelsx.process_results.get_results_df(results_dir, use_cached=False)\n",
    "r = r[[col for col in r.columns if not col in ['checkpoint_verify', 'role_verify']]]\n",
    "r = r[r.use_megaprompt == 0]\n",
    "\n",
    "def viz_blues(df):\n",
    "    return df.style.format(precision=3).background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medication_status\n",
    "r_med = r[r.dataset_name == 'medication_status']\n",
    "df = pd.DataFrame.from_dict(datasets.load_dataset('mitclinicalml/clinical-ie', 'medication_status')['test'])\n",
    "r_med = clin.eval.med_status.add_status_eval(r_med, df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medication_status\n",
    "r_med = r[r.dataset_name == \"medication_status\"]\n",
    "df = pd.DataFrame.from_dict(\n",
    "    datasets.load_dataset(\"mitclinicalml/clinical-ie\", \"medication_status\")[\"test\"]\n",
    ")\n",
    "r_med = clin.eval.med_status.add_status_eval(r_med, df)\n",
    "r_med = imodelsx.process_results.average_over_seeds(\n",
    "    r_med, experiment_filename=join(PATH_REPO, \"experiments\", \"eval_model.py\")\n",
    ")\n",
    "\n",
    "# compare values for a single row\n",
    "row_df = r_med[\n",
    "    (r_med.n_shots == 5) * (r_med.checkpoint == \"text-davinci-003\")\n",
    "].reset_index()\n",
    "rc = row_df[[c for c in row_df.columns if \"___\" in c]]\n",
    "# create multindex columns by splitting on '___'\n",
    "rc = rc.rename(columns={c: tuple(c.split(\"___\")) for c in rc.columns})\n",
    "\n",
    "# convert tuple column names to multiindex\n",
    "rc.columns = pd.MultiIndex.from_tuples(rc.columns)\n",
    "rc = rc.T.reset_index()\n",
    "rc = (\n",
    "    rc.rename(\n",
    "        columns={\n",
    "            \"level_0\": \"\",\n",
    "            \"level_1\": \"Verifiers\",\n",
    "        }\n",
    "    )\n",
    "    .pivot_table(index=\"Verifiers\", columns=\"\", values=0)\n",
    "    .round(3)\n",
    ")\n",
    "# rc.style.format(precision=3).background_gradient(cmap='gray')\n",
    "\n",
    "cols = {\n",
    "    \"f1\": \"F1 (Med)\",\n",
    "    \"precision\": \"Precision (Med)\",\n",
    "    \"recall\": \"Recall (Med)\",\n",
    "    \"status_f1_macro_cond\": \"F1 (Med status)\",\n",
    "}\n",
    "rows = {\n",
    "    \"original\": \"Original\",\n",
    "    \"ov\": \"Omission\",\n",
    "    \"pv\": \"Prune\",\n",
    "    \"ov_pv\": \"Omission + Prune\",\n",
    "    \"sv\": \"Omission + Prune + Evidence\",\n",
    "}\n",
    "rt_med = (\n",
    "    rc[list(cols.keys())].rename(columns=cols).loc[list(rows.keys())].rename(index=rows)\n",
    ")\n",
    "rt_med_sem = (\n",
    "    rc[list(cols.keys())].rename(columns=cols)\n",
    "    .loc[[k + '_err' for k in list(rows.keys())]]\n",
    "    .rename(index={k + '_err': rows[k] for k in rows})\n",
    ")\n",
    "viz_blues(rt_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ebm = r[r.dataset_name == \"ebm\"]\n",
    "r_ebm = imodelsx.process_results.average_over_seeds(\n",
    "    r_ebm, experiment_filename=join(PATH_REPO, \"experiments\", \"eval_model.py\")\n",
    ")\n",
    "\n",
    "# compare values for a single row\n",
    "row = r_ebm[(r_ebm.n_shots == 5) * (r_ebm.checkpoint == \"text-davinci-003\")].iloc[0]\n",
    "\n",
    "# show metrics\n",
    "row_df = pd.DataFrame(\n",
    "    pd.Series({k: row[k] for k in row.keys() if \"___\" in k}).round(3)\n",
    ").T\n",
    "rc = row_df[[c for c in row_df.columns if \"___\" in c]]\n",
    "# create multindex columns by splitting on '___'\n",
    "rc = rc.rename(columns={c: tuple(c.split(\"___\")) for c in rc.columns})\n",
    "\n",
    "# convert tuple column names to multiindex\n",
    "rc.columns = pd.MultiIndex.from_tuples(rc.columns)\n",
    "rc = rc.T.reset_index()\n",
    "rc = (\n",
    "    rc.rename(\n",
    "        columns={\n",
    "            \"level_0\": \"\",\n",
    "            \"level_1\": \"Verifiers\",\n",
    "        }\n",
    "    )\n",
    "    .pivot_table(index=\"Verifiers\", columns=\"\", values=0)\n",
    "    .round(3)\n",
    ")\n",
    "# rc.index = [x.replace(\"list_\", \"\") for x in rc.index.values]\n",
    "cols = {\n",
    "    \"f1\": \"F1 (Arms)\",\n",
    "    \"precision\": \"Precision (Arms)\",\n",
    "    \"recall\": \"Recall (Arms)\",\n",
    "}\n",
    "rows = {\n",
    "    \"original\": \"Original\",\n",
    "    \"ov\": \"Omission\",\n",
    "    \"pv\": \"Prune\",\n",
    "    \"ov_pv\": \"Omission + Prune\",\n",
    "    \"ov_pv_ev\": \"Omission + Prune + Evidence\",\n",
    "}\n",
    "rt_ebm = rc[list(cols.keys())].rename(columns=cols).loc[list(rows.keys())].rename(index=rows)\n",
    "rt_ebm_sem = (\n",
    "    rc[list(cols.keys())].rename(columns=cols)\n",
    "    .loc[[k + '_err' for k in list(rows.keys())]]\n",
    "    .rename(index={k + '_err': rows[k] for k in rows})\n",
    ")\n",
    "viz_blues(rt_ebm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns from rt_ebm to rt_med\n",
    "rt = rt_med.join(rt_ebm) #, rsuffix=' (Arms)')\n",
    "rt = rt.drop(columns='F1 (Med status)')\n",
    "display(viz_blues(rt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add error bars\n",
    "for k in rt_ebm.index:\n",
    "    rt_ebm.loc[k] = rt_ebm.loc[k].astype(str) + \"\\err{\" + rt_ebm_sem.loc[k].astype(str) + \"}\"\n",
    "for k in rt_med.index:\n",
    "    rt_med.loc[k] = (\n",
    "        rt_med.loc[k].astype(str) + \"\\err{\" + rt_med_sem.loc[k].astype(str) + \"}\"\n",
    "    )\n",
    "\n",
    "rt = rt_med.join(rt_ebm) #, rsuffix=' (Arms)')\n",
    "rt = rt.drop(columns='F1 (Med status)')\n",
    "print(rt.style.format(precision=3).to_latex(hrules=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".embgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
