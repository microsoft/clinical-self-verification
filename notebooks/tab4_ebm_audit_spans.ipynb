{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import clin.llm\n",
    "import clin.parse\n",
    "from typing import List, Dict\n",
    "results_dir = '../results/'\n",
    "from clin.config import PATH_REPO\n",
    "import clin.eval.ebm\n",
    "import clin.eval.eval\n",
    "from clin.modules import ebm\n",
    "import joblib\n",
    "import imodelsx.process_results\n",
    "from IPython.display import HTML\n",
    "import clin.viz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<00:00, 1823.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# get human spans\n",
    "df_spans = joblib.load(join(PATH_REPO, 'data', 'ebm', 'ebm_interventions_spans.pkl'))\n",
    "# nums = np.arange(100).tolist()\n",
    "# np.random.default_rng(seed=13).shuffle(nums)\n",
    "# dfe_spans = df_spans.iloc[nums]\n",
    "df_spans = df_spans.iloc[:100]\n",
    "\n",
    "# get predicted evidence\n",
    "r = imodelsx.process_results.get_results_df(results_dir, use_cached=False)\n",
    "r = r[[col for col in r.columns if not col in ['checkpoint_verify', 'role_verify']]]\n",
    "r = r[r.dataset_name == 'ebm']\n",
    "# r = r[(r.n_shots == 5) * (r.checkpoint == 'text-davinci-003')]\n",
    "row = r.iloc[0]\n",
    "\n",
    "# get common keys across each list\n",
    "common_keys = [\n",
    "    set.intersection(\n",
    "        *[set(r[\"dict_evidence_ov_pv_ev\"].iloc[i][j].keys()) for i in range(len(r))]\n",
    "    )\n",
    "    for j in range(100)\n",
    "]\n",
    "r['dict_evidence_common'] = r.apply(lambda x: [{k: x['dict_evidence_ov_pv_ev'][i][k] for k in common_keys[i]} for i in range(100)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_matches(d_evidence: List[Dict[str, str]], df_spans):\n",
    "    \"\"\"Finds mean number of times evidence span from llm contains a token from the human span.\n",
    "    Mean is taken for each document and then averaged over all documents.\n",
    "    Baseline is probability that any token falls into a human span.\n",
    "    \"\"\"\n",
    "\n",
    "    # single example\n",
    "    mean_matches = []\n",
    "    mean_num_tokens = []\n",
    "    for i in range(len(df_spans)):\n",
    "        span = df_spans.iloc[i]\n",
    "        doc = span['doc'].lower()\n",
    "        toks = [tok.lower() for tok in span['toks_list']]\n",
    "        annot = span['annot_list']\n",
    "        # color_str = clin.viz.colorize(span['toks_list'], span['annot_list'], char_width_max=60, title=str(i) + \" \" + span['doc_id'])\n",
    "        # display(HTML(color_str))\n",
    "\n",
    "        # given reference text and set of tokens, find starting index of each token\n",
    "        def _find_token_idxs(doc, toks):\n",
    "            starts = []\n",
    "            ends = []\n",
    "            for tok in toks:\n",
    "                idx = doc.find(tok, ends[-1] if len(ends) > 0 else 0)\n",
    "                if idx == -1:\n",
    "                    print('ERROR: token not found:', tok)\n",
    "                    return None\n",
    "                starts.append(idx)\n",
    "                ends.append(idx + len(tok))\n",
    "            \n",
    "            # check that idxs are strictly increasing\n",
    "            for i in range(1, len(starts)):\n",
    "                if starts[i] <= starts[i - 1]:\n",
    "                    print('ERROR: idxs not strictly increasing')\n",
    "                    return None\n",
    "            return starts, ends\n",
    "        starts, ends = _find_token_idxs(doc, toks)\n",
    "\n",
    "        def _get_overlapping_token_idxs(start: int, end: int, starts: List[int], ends: List[int]):\n",
    "            \"\"\"\n",
    "            Given a span [start, end), find the indices of all tokens that overlap with the span.\n",
    "            \"\"\"\n",
    "            idxs = []\n",
    "            for i in range(len(starts)):\n",
    "                if start < ends[i] and end > starts[i]:\n",
    "                    # print(start, end, starts[i], ends[i])\n",
    "                    idxs.append(i)\n",
    "            return idxs\n",
    "\n",
    "        matches = []\n",
    "        num_toks = []\n",
    "        for intervention_name, intervention_evidence in d_evidence[i].items():\n",
    "            s = re.escape(intervention_evidence.lower())\n",
    "            # s = intervention_name.lower()\n",
    "\n",
    "            # search over all matches\n",
    "            idxs_match = [m.start() for m in re.finditer(s, doc)]\n",
    "            for idx_match in idxs_match:\n",
    "                tok_idxs = _get_overlapping_token_idxs(idx_match, idx_match + len(s), starts, ends)\n",
    "                matches.append(np.any(annot[tok_idxs] > 0))\n",
    "                # num_toks.append(len(tok_idxs))\n",
    "                num_toks.append(len(s.split()))\n",
    "            # else:\n",
    "                # matches.append(0)\n",
    "                # num_toks.append(len(s.split())\n",
    "        mean_matches.append(np.nanmean(matches))\n",
    "        mean_num_tokens.append(np.nanmean(num_toks))\n",
    "    return np.nanmean(mean_matches), np.nanmean(mean_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_783453/911155811.py:65: RuntimeWarning: Mean of empty slice\n",
      "  mean_matches.append(np.nanmean(matches))\n",
      "/tmp/ipykernel_783453/911155811.py:66: RuntimeWarning: Mean of empty slice\n",
      "  mean_num_tokens.append(np.nanmean(num_toks))\n",
      "/tmp/ipykernel_783453/911155811.py:65: RuntimeWarning: Mean of empty slice\n",
      "  mean_matches.append(np.nanmean(matches))\n",
      "/tmp/ipykernel_783453/911155811.py:66: RuntimeWarning: Mean of empty slice\n",
      "  mean_num_tokens.append(np.nanmean(num_toks))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random baseline 0.03757215007215007\n"
     ]
    }
   ],
   "source": [
    "r['Span overlap accuracy'] = r.apply(lambda row: calculate_mean_matches(row['dict_evidence_ov_pv_ev'], df_spans)[0], axis=1)\n",
    "r['Span length'] = r.apply(lambda row: calculate_mean_matches(row['dict_evidence_ov_pv_ev'], df_spans)[1], axis=1)\n",
    "print('random baseline', np.concatenate(df_spans['annot_list'].values).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dataset_name', 'seed', 'save_dir', 'checkpoint', 'n_shots',\n",
       "       'use_cache', 'save_dir_unique', 'extracted_strs', 'recall___original',\n",
       "       'precision___original', 'f1___original', 'dict_original', 'recall___ov',\n",
       "       'precision___ov', 'f1___ov', 'dict_ov', 'recall___pv', 'precision___pv',\n",
       "       'f1___pv', 'dict_pv', 'recall___ev', 'precision___ev', 'f1___ev',\n",
       "       'dict_ev', 'recall___ov_pv', 'precision___ov_pv', 'f1___ov_pv',\n",
       "       'dict_ov_pv', 'recall___ov_pv_ev', 'precision___ov_pv_ev',\n",
       "       'f1___ov_pv_ev', 'dict_ov_pv_ev', 'recall___sv', 'precision___sv',\n",
       "       'f1___sv', 'dict_sv', 'dict_evidence_ov_pv_ev', 'list_original',\n",
       "       'list_ov', 'list_pv', 'list_ov_pv', 'list_ov_pv_ev',\n",
       "       'dict_evidence_common', 'Span overlap accuracy', 'Span length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Span overlap accuracy</th>\n",
       "      <th>Span length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checkpoint</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>0.87</td>\n",
       "      <td>14.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0314</th>\n",
       "      <td>0.93</td>\n",
       "      <td>8.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-003</th>\n",
       "      <td>0.84</td>\n",
       "      <td>7.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Span overlap accuracy  Span length\n",
       "checkpoint                                          \n",
       "gpt-3.5-turbo                      0.87        14.30\n",
       "gpt-4-0314                         0.93         8.20\n",
       "text-davinci-003                   0.84         7.33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab = (\n",
    "    r\n",
    "    .loc[r['n_shots'] == 5]\n",
    "    .groupby('checkpoint')[['Span overlap accuracy', 'Span length']].mean().round(2)\n",
    ")\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imodelsx.viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab.index = tab.index.map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      " & Span overlap accuracy & Span length \\\\\n",
      "checkpoint &  &  \\\\\n",
      "\\midrule\n",
      "ChatGPT & 0.87 & 14.30 \\\\\n",
      "GPT-4 & 0.93 & 8.20 \\\\\n",
      "GPT-3 & 0.84 & 7.33 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tab.style.format(precision=2).to_latex(hrules=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".embgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
