{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install openai langchain  tiktoken  pinecone-client openpyxl  sentence-transformers pinecone-text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict \n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zelalemgero\\AppData\\Local\\anaconda3\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zelalemgero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zelalemgero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import VectorDBQAWithSourcesChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import LLMRequestsChain, LLMChain\n",
    "\n",
    "import tiktoken\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openai api key config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "openai.api_base = \"YOUR_API_BASE\"\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version ='2023-03-15-preview'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the preprocessed mimic III file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/PATH/TO/YOUR/FILE/mimiciii_sampled.pkl'\n",
    "sampled_mimiciii = pd.read_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sampled_mimiciii[['_id','text','target']]\n",
    "mimiciii_dict = dict(zip(df['_id'],df['target']))\n",
    "\n",
    "token_text = df.iloc[1]['text']\n",
    "chunks = 800\n",
    "new_splits = []\n",
    "for idx,row in df.iterrows():\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, chunk_overlap=50, length_function=tiktoken_len, separators=[\"\\n\\n\",\"\\n\", \" \",\"\"]) \n",
    "  txt = row['text'].replace('\\n', ' ').replace('[**', '').replace('**]','').replace('*', '').replace('--','').replace('__','')\n",
    "  txt = re.sub('\\s{2,}', ' ', txt) \n",
    "  splits = text_splitter.split_text(txt)\n",
    "  new_splits.extend([[x,row['_id'],'NA'] for x in splits])\n",
    "  if len(new_splits) > 10000:\n",
    "    break\n",
    "\n",
    "df2 = pd.DataFrame(new_splits, columns =['text', 'index','code']) \n",
    "test_keys = list(set(df2['index']))\n",
    "\n",
    "bm25 = BM25Encoder()\n",
    "bm25.fit(token_text)\n",
    "embd_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a pinecone vector database and insert the chunked mimic notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=pinecone_key, environment=pinecone_env)\n",
    "\n",
    "index_name = \"mimic-search\"\n",
    "\n",
    "# check if the ner-search index exists\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # create the index if it does not exist\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=384,\n",
    "        metric=\"dotproduct\",\n",
    "        pod_type=\"s1\"\n",
    "    )\n",
    "# connect to doc-search index we created\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def insert_text(df):\n",
    "  # we will use batches of 64\n",
    "  batch_size = 8\n",
    "  #EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "  \n",
    "\n",
    "  for i in tqdm(range(0, len(df), batch_size)):\n",
    "      ids =  [x for x in range(batch_size)]\n",
    "      # find end of batch\n",
    "      i_end = min(i+batch_size, len(df))\n",
    "      # extract batch\n",
    "      batch = df.iloc[i:i_end]\n",
    "      # generate embeddings for batch\n",
    "      #dense_vecs = embed_model.encode().tolist()\n",
    "      #dense_vecs = [get_embedding(txt,  EMBEDDING_MODEL) for txt in batch[\"text\"].tolist()]\n",
    "      dense_vecs = embd_model.encode(batch[\"text\"].tolist())\n",
    "      sparse_vecs = bm25.encode_documents(batch[\"text\"].tolist())\n",
    "      # create sparse vecs\n",
    "      contexts = batch['text'].tolist()\n",
    "      metadata = batch.to_dict(orient=\"records\")\n",
    "      upserts = []\n",
    "      for _id, sparse, dense,  context in zip(ids, sparse_vecs,dense_vecs, contexts):\n",
    "          # build metadata struct\n",
    "          #metadata = {'context': context}\n",
    "          meta = metadata[_id]\n",
    "          #batch = batch.drop('text', axis=1)\n",
    "          \n",
    "          # append all to upserts list as pinecone.Vector (or GRPCVector)\n",
    "          upserts.append({\n",
    "              'id': 'note' + str(_id) + str(i),\n",
    "              'sparse_values': sparse,\n",
    "              'values': dense,\n",
    "              'metadata': meta})\n",
    "\n",
    "      _ = index.upsert(vectors=upserts)\n",
    "\n",
    "      if i % 100 == 0:\n",
    "        print(i)\n",
    "      \n",
    "\n",
    "  index.describe_index_stats() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert text notes into vetor db\n",
    "insert_text(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query(text: str):\n",
    "    # create dense vec\n",
    "    dense_vec = embd_model.encode(text).tolist()\n",
    "    #dense_vec = get_embedding(text, engine=EMBEDDING_MODEL)\n",
    "\n",
    "    # create sparse vec\n",
    "    sparse_vec = bm25.encode_queries(text)\n",
    "    return dense_vec, sparse_vec\n",
    "\n",
    "\n",
    "\n",
    "def hybrid_scale(dense, sparse, alpha: float):\n",
    "    # check alpha value is in range\n",
    "    if alpha < 0 or alpha > 1:\n",
    "        raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "    # scale sparse and dense vectors to create hybrid search vecs\n",
    "    hsparse = {\n",
    "        'indices': sparse['indices'],\n",
    "        'values':  [v * (1 - alpha) for v in sparse['values']]\n",
    "    }\n",
    "    hdense = [v * alpha for v in dense]\n",
    "    return hdense, hsparse\n",
    "\n",
    "def search_pinecone(query,alpha=1):\n",
    "    if len(query)==2:\n",
    "      q = query[0] \n",
    "      filter_criteria = {\"index\": {\"$eq\": query[1]}}\n",
    "    elif len(query) == 1:\n",
    "       q = query\n",
    "       filter_criteria = None\n",
    "    else:\n",
    "       raise Exception('Please input a query') \n",
    "    dense,sparse = encode_query(query[0])  \n",
    "    hdense, hsparse = hybrid_scale(dense,sparse, alpha= alpha)\n",
    "   \n",
    "    # create embeddings for the query\n",
    "    xc = index.query(top_k=150, vector=hdense,sparse_vector=hsparse,include_metadata=True, filter=filter_criteria)\n",
    "    #print(xc)\n",
    "    indx_score = {}\n",
    "    for lst in xc['matches']:\n",
    "      idx = lst['metadata']['index']\n",
    "      if idx == 'icd':\n",
    "        idx = 'icd_' + lst['metadata']['code']\n",
    "      score = lst['score']\n",
    "      indx_score[idx] = score\n",
    "    \n",
    "    r = [x[\"metadata\"] for x in xc[\"matches\"]]\n",
    "    return {\"index score\": indx_score, \"metadata\": r}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tiktoken tokenizer for adjusting number of tokens passed to various openai models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tik_tokenizer = tiktoken.get_encoding('p50k_base')\n",
    "def tiktoken_len(text):\n",
    "  tokens = tik_tokenizer.encode(text, disallowed_special = ())\n",
    "  return len(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create schemas for prompting using langchain's ResponseSchema module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_schema = ResponseSchema(name = 'diseases', description=\"this is focuses on medical history of the patient.It involves extracting information about diseases, disorders, or medical conditions that have affected the patient\")\n",
    "response_schemas = [disease_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "template_string = \"\"\"You are an expert in information extraction from clinical notes. Given a clinical note, you extract diseases/disorders/procedures and return all as a single python list of strings like [\"disease\",\"disease\",\"procedure\"]. Here is the note: {text_note}. {format_instructions}\n",
    "\"\"\"\n",
    "disease_prompt = ChatPromptTemplate(messages=[HumanMessagePromptTemplate.from_template(template_string)],\n",
    "                            input_variables=['text_note'],\n",
    "                            partial_variables={\"format_instructions\":format_instructions},\n",
    "                            output_parser=output_parser\n",
    ")\n",
    "\n",
    "evidence_schema = ResponseSchema(name = 'evidence', description=\"\"\"You are an expert fact checker. Your job is to fact check the {diseases} provided based on the full input text given and return a python list of lists. Individual lists have\n",
    "                                            evidence text span next to each of the disease. Your job is to determine whether each of the values in the list {diseases} is correct or not. Find the span of text (a one sentence) from the \n",
    "                                            {text_input} as an evidence.  Add the text span evidence that makes the answer True or False. Dont add any extra text.\n",
    "                                             Example output is here ###\n",
    "                                              [\n",
    "                                             [\"ESRD\",  \"ESRD secondary to hypertensive nephrosclerosis \",\"True\"],\n",
    "                                            [\"DM\", \" DM, on glipizide at home\",\"True\"],\n",
    "                                            [\"Hypertension\", \"high blood pressure ruled out\", \"False\"]\n",
    "                                                        ]                               \"\"\")\n",
    "response_schemas = [evidence_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "template_string = \"\"\"You are an expert fact checker. Given a clinical note and list of diseases and procedures, you verify by extracting evidence and return all as a single python list of strings. Here is the note: {text_note} and list of diseases and procedures. {format_instructions}\n",
    "\"\"\"\n",
    "evidence_prompt = ChatPromptTemplate(messages=[HumanMessagePromptTemplate.from_template(template_string)],\n",
    "                            input_variables=['text_note','diseases'],\n",
    "                            partial_variables={\"format_instructions\":format_instructions},\n",
    "                            output_parser=output_parser\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "entail_schema = ResponseSchema(name = 'entail', description=\"this focuses on checking wether the disease/procedure can be entailed from the text fragment in the provided {text_note} in the format [disease, text fragment] \")\n",
    "response_schemas = [entail_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "template_string = \"\"\" You are an expert textual entailment agent. Textual Entailment(aka Natural Language inference) is directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. Your job is to check wether the disease can be entailed from the text in the provided {text_note} in the format (disease, text fragment).Dont extrapolate, entail only based on the provided text fragment. If the disease can be entailed from the text fragment based on the {text_note}, add the diseases to the output list. Return the disease list of all diseases that can be entailed. All values must be in a string format inside double quotations . Here is the note: {text_note}. {format_instructions}\n",
    "\"\"\"\n",
    "entail_prompt = ChatPromptTemplate(messages=[HumanMessagePromptTemplate.from_template(template_string)],\n",
    "                            input_variables=['text_note'],\n",
    "                            partial_variables={\"format_instructions\":format_instructions},\n",
    "                            output_parser=output_parser\n",
    ")\n",
    "\n",
    "omissions_schema = ResponseSchema(name = 'omissions', description=\"this focuses on finding all the missing disease/procedure from the provided {text_note} that are not in the list {diseases} \")\n",
    "response_schemas = [omissions_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "template_string = \"\"\" You are an expert disease inspector. Your job is to find all possible diseases/procedures in the given {text_note} exhaustively and return in a python list of strings. Your response should be in the form of \n",
    "                                            python list with all the diseases/procedures that you can verify do exist in the {text_note}. Make sure to return the disease/procedures exhaustively.Dont include a disease/procedure if it is in the {diseases} list. Return only unique diseases/procedures. All diseases/procedures in the list must be in a string format.  Here is the note: {text_note} and the list of diseases/procedures {diseases}. {format_instructions}\n",
    "\"\"\"\n",
    "omissions_prompt = ChatPromptTemplate(messages=[HumanMessagePromptTemplate.from_template(template_string)],\n",
    "                            input_variables=['text_note','diseases'],\n",
    "                            partial_variables={\"format_instructions\":format_instructions},\n",
    "                            output_parser=output_parser\n",
    ")\n",
    "\n",
    "\n",
    "icd_schema = ResponseSchema(name = 'icd', description=\"this focuses on assining ICD 9 codes for all the diseases/procedures listed in the {diseases} based on the context in {text_note} \")\n",
    "response_schemas = [icd_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "template_string = \"\"\"You are an expert clinical data encoder.  your job is to extract ICD 9 codes for all the diseases/procedures listed in {diseases} based ont the context in {text_note}. Then return a python list of strings containing all the ICD 9 codes you assigned.{format_instructions}\n",
    "\"\"\"\n",
    "icd_prompt = ChatPromptTemplate(messages=[HumanMessagePromptTemplate.from_template(template_string)],\n",
    "                            input_variables=['text_note','diseases'],\n",
    "                            partial_variables={\"format_instructions\":format_instructions},\n",
    "                            output_parser=output_parser\n",
    ")\n",
    "\n",
    "icd_schema_10 = ResponseSchema(name = 'icd', description=\"this focuses on assining ICD 10 codes for all the diseases/procedures listed in the {diseases} based on the context in {text_note} \")\n",
    "response_schemas = [icd_schema_10]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "template_string = \"\"\"You are an expert clinical data encoder.  your job is to extract ICD 10 codes for all the diseases/procedures listed in {diseases} based ont the context in {text_note}. Then return a python list of strings containing all the ICD 10 codes you assigned.{format_instructions}\n",
    "\"\"\"\n",
    "icd_prompt_10 = ChatPromptTemplate(messages=[HumanMessagePromptTemplate.from_template(template_string)],\n",
    "                            input_variables=['text_note','diseases'],\n",
    "                            partial_variables={\"format_instructions\":format_instructions},\n",
    "                            output_parser=output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diseases(note):\n",
    "  for i in range(3):\n",
    "    try:\n",
    "      chain = LLMChain(llm=llm,prompt=disease_prompt)\n",
    "      diseases = chain.predict_and_parse(text_note = note)\n",
    "      return diseases['diseases']\n",
    "    except openai.error.RateLimitError:\n",
    "      time.sleep(5)  \n",
    "    except Exception as e:\n",
    "      print(e) \n",
    "\n",
    "def get_evidence(note,diseases):\n",
    "  for i in range(3):\n",
    "    try:\n",
    "      chain = LLMChain(llm=llm,prompt=evidence_prompt)\n",
    "      evidence = chain.predict_and_parse(text_note = note, diseases = diseases)\n",
    "      return evidence  \n",
    "    except openai.error.RateLimitError:\n",
    "      time.sleep(5)  \n",
    "    except Exception as e:\n",
    "      print(e) \n",
    "\n",
    "def does_ential(evidence):\n",
    "  for i in range(3):\n",
    "    try:\n",
    "      chain = LLMChain(llm=llm,prompt=entail_prompt)\n",
    "      evidence_list = [(x[0],x[1]) for x in evidence['evidence'] if x[2].lower()=='true']\n",
    "      entail = chain.predict_and_parse( text_note = evidence_list)\n",
    "      verified = entail['entail']\n",
    "      return verified  \n",
    "    except openai.error.RateLimitError:\n",
    "      time.sleep(5)  \n",
    "    except Exception as e:\n",
    "      print(e) \n",
    "\n",
    "def find_omissions(note, verified):\n",
    "  for i in range(3):\n",
    "    try:\n",
    "      chain = LLMChain(llm=llm,prompt=omissions_prompt)\n",
    "      omissions = chain.predict_and_parse( text_note = note, diseases = verified)\n",
    "      return omissions['omissions']  \n",
    "    except openai.error.RateLimitError:\n",
    "      time.sleep(5)  \n",
    "    except Exception as e:\n",
    "      print(e)   \n",
    "\n",
    "def get_icds(note, diseases):\n",
    "  for _ in range(3):\n",
    "    try:\n",
    "      chain = LLMChain(llm=llm,prompt=icd_prompt)\n",
    "      icds = chain.predict_and_parse( text_note = note, diseases = diseases)\n",
    "      return icds\n",
    "    except openai.error.RateLimitError:\n",
    "      time.sleep(5)  \n",
    "    except Exception as e:\n",
    "      print(e)   \n",
    "\n",
    "def get_icds_10(note, diseases):\n",
    "  for _ in range(3):\n",
    "    try:\n",
    "      chain = LLMChain(llm=llm,prompt=icd_prompt_10)\n",
    "      icds = chain.predict_and_parse( text_note = note, diseases = diseases)\n",
    "      return icds\n",
    "    except openai.error.RateLimitError:\n",
    "      time.sleep(5)  \n",
    "    except Exception as e:\n",
    "      print(e)  \n",
    "\n",
    "def extract_notes(k):\n",
    "  query = [ \"disease, patient conditions, Diagnosis, treatment, examination, laboratory tests, imaging, medical problem, medical condition past medical history, what are the diseases/conditions the patient had, procedures ?\",indx]\n",
    "  result = search_pinecone(query,0.7)\n",
    "  retrieved_text = \"\" .join([x['text'] for x in result['metadata']])[:400000] \n",
    "  \n",
    "  return retrieved_text   \n",
    "\n",
    "def calculate_metrics_9(output, icd):\n",
    "    tn, fn, fp, acc, total_pred, total_y = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    for k, vv in output.items():\n",
    "        try:\n",
    "          new_v = vv['icd']\n",
    "        except:\n",
    "          continue  \n",
    "        y_true = {str(x) for x in mimiciii_dict[k] if str(x) in icd}\n",
    "        if not y_true:\n",
    "          continue\n",
    "        v = {xx.strip() for xx in new_v if xx.strip() in icd}\n",
    "        \n",
    "        neg = set(icd).difference(y_true)\n",
    "        potential_neg = set(icd).difference(v)\n",
    "\n",
    "        tn += len(neg.intersection(potential_neg))\n",
    "        fn += len(potential_neg.intersection(y_true))\n",
    "        fp += len(v.intersection(neg))\n",
    "        total_y += len(y_true)\n",
    "        acc += len(v.intersection(y_true))\n",
    "        total_pred += len(v)\n",
    "\n",
    "    P = acc / total_pred\n",
    "    R = acc / total_y\n",
    "    A = (acc + tn) / (acc + tn + fn + fp)\n",
    "\n",
    "    #return {'P': P, 'R': R, 'A': A}\n",
    "\n",
    "    print(\"total correct:\", acc, \"total pred:\",total_pred, \"total true:\", total_y)  \n",
    "    print('precision is ',P)\n",
    "    print('recall is ',R)  \n",
    "    print('F1 is ',2*((P*R)/(P+R)))\n",
    "    print('Accuracy is ',A)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 10 and top50 most common ICD9 codes for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_top10 = ['401.9','272.4','530.81','250.00','428.0','427.31','414.01','518.81','599.0','584.9']\n",
    "icd_top50 = ['401.9','38.93','428.0','427.31','414.01','96.04','96.6','584.9','250.00','96.71','272.4','518.81','99.04','39.61','599.0','530.81','96.72','272.0', '285.9','88.56','244.9','486','38.91', '285.1','36.15','276.2','496','99.15','995.92','V58.61','507.0','038.9','88.72','585.9','403.90','311','305.1','37.22','412','33.24','39.95','287.5','410.71','276.1','V45.81','424.0', '45.13','V15.82','511.9','37.23']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output variables for storing the abalation runs \n",
    "base_icd = defaultdict(list)\n",
    "prune_icd = defaultdict(list)\n",
    "omissions_icd = defaultdict(list)\n",
    "op_icd = defaultdict(list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the openai models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_4 = ChatOpenAI(model_name = 'gpt-4-32k-0314', openai_api_key=openai.api_key,temperature=0.1,max_tokens=500, deployment_id='gpt-4-32k-0314')\n",
    "llm_35 = ChatOpenAI(model_name = 'gpt-35-turbo-0301', openai_api_key=openai.api_key,temperature=0.1,max_tokens=500,deployment_id='gpt-35-turbo-0301')\n",
    "llm_3 = OpenAI(model_name = 'text-davinci-003', openai_api_key=openai.api_key,temperature=0.1,max_tokens=500, deployment_id='text-davinci-003')\n",
    "\n",
    "llm_list = [llm_4,llm_35,llm_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIMIC III ICD-9 coding\n",
    "for model in llm_list:\n",
    "  llm = model\n",
    "  \n",
    "  for k in test_keys:\n",
    "    diseases,entail,all_omissions = [],[],[]\n",
    "    note = extract_notes(k)\n",
    "\n",
    "    # adjust maximum context length for each model\n",
    "    num_tokens = tiktoken_len(note)\n",
    "    if llm.model_name == 'text-davinci-003':\n",
    "      max_tokens = 3000\n",
    "    elif llm.model_name ==  'gpt-35-turbo-0301':\n",
    "      max_tokens = 7000\n",
    "    else:\n",
    "      max_tokens = 30000  \n",
    "    while num_tokens > max_tokens:\n",
    "      delta =  num_tokens - max_tokens\n",
    "      note = note[:-delta*3]\n",
    "      num_tokens = tiktoken_len(note)\n",
    "    \n",
    "\n",
    "    #Extract diseases\n",
    "    diseases = get_diseases(note)\n",
    "    if diseases:\n",
    "      icds = get_icds(note, diseases)\n",
    "      base_icd[k] = icds\n",
    "    else:\n",
    "      base_icd[k] = []  \n",
    "    \n",
    "    #Generate Evidence \n",
    "    evidence = get_evidence(note,diseases)\n",
    "    entail = does_ential(evidence)\n",
    "    if entail:\n",
    "      icds = get_icds(note,entail)\n",
    "      prune_icd[k] = icds\n",
    "    else:\n",
    "      prune_icd[k] = []  \n",
    "    \n",
    "    \n",
    "    #Find omissions\n",
    "    if diseases:\n",
    "      all_omissions = diseases.copy()\n",
    "    else:\n",
    "      all_omissions = []  \n",
    "    for _ in range(3):\n",
    "      \n",
    "      omissions = find_omissions(note,all_omissions)\n",
    "      if not omissions:\n",
    "        break\n",
    "      all_omissions.extend(omissions)\n",
    "    if all_omissions:  \n",
    "      icds = get_icds(note, all_omissions) \n",
    "      omissions_icd[k] = icds\n",
    "    else:\n",
    "      omissions_icd[k] = []  \n",
    "    \n",
    "\n",
    "    # Verify found omissions\n",
    "    evidence = get_evidence(note,all_omissions)\n",
    "    entail = does_ential(evidence)\n",
    "    if entail:\n",
    "      icds = get_icds(note,  entail)\n",
    "      op_icd[k] = icds\n",
    "    else:\n",
    "      op_icd[k] = []  \n",
    "    \n",
    "\n",
    "\n",
    "  print(f'Top 10 Extractions Using {llm.model_name} model:')\n",
    "  icd = icd_top10\n",
    "  \n",
    "  print(\"base \",calculate_metrics_9(omissions_icd, icd))\n",
    "  print(\"prune \",calculate_metrics_9(prune_icd, icd))\n",
    "  print(\"omissions \",calculate_metrics_9(omissions_icd, icd))\n",
    "  print(\" omissions + prune\",calculate_metrics_9(op_icd, icd))  \n",
    "\n",
    "  print(f'Top 50 Extractions Using {llm.model_name} model:')\n",
    "  icd = icd_top50\n",
    "  print(\"base \",calculate_metrics_9(base_icd, icd))\n",
    "  print(\"prune \",calculate_metrics_9(prune_icd, icd))\n",
    "  print(\"omissions \",calculate_metrics_9(omissions_icd, icd))\n",
    "  print(\" omissions + prune\",calculate_metrics_9(op_icd, icd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimiciii_icd_pred = {}\n",
    "for kk,vv in op_icd.items():\n",
    "  mimiciii_icd_pred[kk] = {\"base\":base_icd[kk], \"prune\":prune_icd[kk],\"omissions\":omissions_icd[kk],\"op\":vv}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"YOUR/OUTUT/PATH\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok = True)\n",
    "with open(filename,'w') as outfile:\n",
    "  json.dump(mimiciii_icd_pred, outfile, indent=4, sort_keys = True, default = str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
