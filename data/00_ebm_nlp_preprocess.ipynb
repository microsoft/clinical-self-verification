{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import imodelsx.process_results\n",
    "import sys\n",
    "import datasets\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import clin.eval\n",
    "import clin.modules.prune\n",
    "import clin.modules.evidence\n",
    "import clin.modules.omission\n",
    "import clin.modules.status\n",
    "import clin.llm\n",
    "import clin.parse\n",
    "from collections import defaultdict\n",
    "import openai\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import os.path\n",
    "from os.path import join\n",
    "import string\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from IPython.display import display, HTML\n",
    "import clin.viz\n",
    "import joblib\n",
    "openai.api_key_path = '/home/chansingh/.OPENAI_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract data\n",
    "# !wget https://github.com/bepnye/EBM-NLP/raw/master/ebm_nlp_2_00.tar.gz\n",
    "# !tar -xvf ebm_nlp_2_00.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'ebm_nlp_2_00'\n",
    "DOC_DIR = join(DATA_DIR, 'documents')\n",
    "ANNOT_DIR = join(DATA_DIR, 'annotations', 'aggregated', 'starting_spans', 'interventions', 'test', 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_fnames = os.listdir(ANNOT_DIR)\n",
    "doc_ids_gold = sorted([fname.split('.')[0] for fname in annot_fnames])\n",
    "doc_names = sorted(os.listdir(join(DATA_DIR, 'documents')))\n",
    "doc_ids = sorted(list(set([doc_name.split('.')[0] for doc_name in doc_names])))\n",
    "# find doc_ids that are in doc_ids_gold\n",
    "doc_ids = [doc_id for doc_id in doc_ids if doc_id in doc_ids_gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_and_interventions(doc_id):\n",
    "    doc = open(join(DOC_DIR, doc_id + '.txt'), 'r').read()\n",
    "    toks = open(join(DOC_DIR, doc_id + '.tokens'), 'r').read()\n",
    "    annot = open(join(ANNOT_DIR, doc_id + '.AGGREGATED.ann'), 'r').read()\n",
    "    \n",
    "    toks_list = toks.split()\n",
    "    annot_list = np.array([int(i) for i in annot.split()]).astype(int)\n",
    "    annot_arr = (annot_list > 0).astype(int)\n",
    "\n",
    "    def find_continguous_sequences(annot_arr: np.ndarray) -> List[Tuple]:\n",
    "        # return a list of (start, stop) tuples identifying continguous sequences where annot_list is 1\n",
    "        # e.g. annot_arr = [0, 1, 1, 0, 1, 1, 1, 0, 0, 1] -> [(1, 3), (4, 7), (9, 11)]\n",
    "        annot_arr = np.concatenate([[0], annot_arr, [0]])\n",
    "        diffs = np.diff(annot_arr)\n",
    "        starts = np.where(diffs == 1)[0]\n",
    "        stops = np.where(diffs == -1)[0]\n",
    "        return list(zip(starts, stops))\n",
    "\n",
    "    contiguous_seqs = find_continguous_sequences(annot_arr)\n",
    "\n",
    "    # get all interventions\n",
    "    interventions = []\n",
    "    unique_interventions = []\n",
    "    for start, stop in contiguous_seqs:\n",
    "        intervention = toks_list[start:stop]\n",
    "        if intervention[-1] in string.punctuation:\n",
    "            intervention = intervention[:-1]\n",
    "        if intervention[0] in string.punctuation:\n",
    "            intervention = intervention[1:]\n",
    "        interventions.append(' '.join(intervention))\n",
    "        iv_lower = ' '.join(intervention).lower()\n",
    "        iv_lower_s = iv_lower + 's'\n",
    "        iv_lower_without_s = iv_lower[:-1] if iv_lower.endswith('s') else iv_lower\n",
    "        unique_interventions_lower = [x.lower() for x in unique_interventions]\n",
    "        if not iv_lower in unique_interventions_lower \\\n",
    "            and not iv_lower_s in unique_interventions_lower\\\n",
    "                and not iv_lower_without_s in unique_interventions_lower:\n",
    "            unique_interventions.append(' '.join(intervention))\n",
    "\n",
    "    return doc, unique_interventions\n",
    "\n",
    "docs_and_interventions = [get_doc_and_interventions(doc_id) for doc_id in tqdm(doc_ids)]\n",
    "docs = [doc for doc, _ in docs_and_interventions]\n",
    "interventions = [intervention for _, intervention in docs_and_interventions]\n",
    "df = pd.DataFrame.from_dict({'doc_id': doc_ids, 'interventions': interventions, 'doc': docs})\n",
    "df.to_csv('ebm_interventions_gold_raw.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    doc_id = df.iloc[i]['doc_id']\n",
    "\n",
    "    doc = open(join(DOC_DIR, doc_id + '.txt'), 'r').read()\n",
    "    toks = open(join(DOC_DIR, doc_id + '.tokens'), 'r').read()\n",
    "    annot = open(join(ANNOT_DIR, doc_id + '.AGGREGATED.ann'), 'r').read()\n",
    "\n",
    "    toks_list = toks.split()\n",
    "    annot_list = np.array([int(i) for i in annot.split()]).astype(float)/2\n",
    "\n",
    "    color_str = clin.viz.colorize(toks_list, annot_list, char_width_max=100, title=str(i) + \" \" + doc_id)\n",
    "    display(HTML(color_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste this dict to start filling in the annotations\n",
    "# d = {doc_ids[i]: interventions[i] for i in range(len(doc_ids))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebm_interventions_labels_cleaned import ANNOTS\n",
    "annot_doc_ids = sorted(list(ANNOTS.keys()))\n",
    "n_clean = len(ANNOTS)\n",
    "df_cleaned_rows = defaultdict(list)\n",
    "for i in range(n_clean):\n",
    "    doc_id = annot_doc_ids[i]\n",
    "    row = df.iloc[i]\n",
    "    assert row['doc_id'] == doc_id, f'{row[\"doc_id\"]} != {doc_id}'\n",
    "    df_cleaned_rows['doc_id'].append(doc_id)\n",
    "    df_cleaned_rows['doc'].append(row['doc'])\n",
    "    df_cleaned_rows['interventions'].append(ANNOTS[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.DataFrame.from_dict(df_cleaned_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ebm_interventions_cleaned.pkl']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df_cleaned, 'ebm_interventions_cleaned.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".embgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
